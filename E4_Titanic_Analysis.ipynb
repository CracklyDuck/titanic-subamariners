{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rm1XXDHbtEWx",
        "-lCPKFBTtMl8",
        "toEBD7cRtYQA",
        "7kw2I17Wkksa",
        "nZ3kujtJTTYz"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Titanic: Dataset Analysis for Survival Prediction**\n",
        "### Equipo 4:\n",
        "\n",
        "*   Fabian Trejo\n",
        "*   Miguel Bermea\n",
        "*   Eduardo Martinez\n",
        "*   Samantha Guanipa\n",
        "*   Francia Garc√≠a\n",
        "*   Alexia Naredo\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jj3iXG0Zaun_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Libraries and Data Loading**"
      ],
      "metadata": {
        "id": "S3bcaQHNlr2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "V6SVpjHgcLP5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function of minmax scaling that returns a new dataframe\n",
        "def minmax(value, min_value, max_value):\n",
        "    return (value - min_value) / (max_value - min_value)\n",
        "\n",
        "def minmax_scaling(dataframe):\n",
        "    dataframe_scale = pd.DataFrame()\n",
        "\n",
        "    for column in dataframe.columns:\n",
        "        min_value = dataframe[column].min()\n",
        "        max_value = dataframe[column].max()\n",
        "        dataframe_scale[column] = dataframe[column].apply(lambda x: minmax(x, min_value, max_value))\n",
        "\n",
        "    return dataframe_scale"
      ],
      "metadata": {
        "id": "T6vbMumVoivb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def heatMap(corr):\n",
        "    # Generate a mask for the upper triangle\n",
        "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "    # Set up the matplotlib figure\n",
        "    f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "    # Generate a custom diverging colormap\n",
        "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "\n",
        "    # Draw the heatmap with the mask and correct aspect ratio\n",
        "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "metadata": {
        "id": "w8WnhFCSrC8w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mifxppVXWWQY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "4ec8d92f-b902-40fb-88ad-ec0a2548f6e6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-bbd59b6a3440>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    130\u001b[0m   )\n\u001b[1;32m    131\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = '/content/gdrive/My Drive/datasets/Titanic/train.csv'\n",
        "#'/content/gdrive/My Drive/datasets/Titanic/train.csv'#'train.csv'\n",
        "# '/content/gdrive/My Drive/Inteligencia Artificial/reto/train.csv'\n",
        "#'/content/gdrive/My Drive/datasets/Titanic/train.csv'\n",
        "df = pd.read_csv(train_set)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3nu60zf0Ws8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jWo1rrxTbGC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exploratory Analysis**"
      ],
      "metadata": {
        "id": "amO94oKglyCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(5)"
      ],
      "metadata": {
        "id": "V6OkF5fIe0f6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# General Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "X-vvDoLeZMVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset size\n",
        "print('Columns:', df.shape[1])\n",
        "print(list(df.columns))\n",
        "print('Rows:', df.shape[0])\n"
      ],
      "metadata": {
        "id": "wiLZsNL9Zj39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'Name' column contains information about the title of the person, and that the 'Cabin' column contains Section and Number of a cabin. New columns will be created to separate this information for analysis."
      ],
      "metadata": {
        "id": "Td3dVThAOaCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new column with only title\n",
        "import re\n",
        "\n",
        "# Funci√≥n para extraer el texto deseado\n",
        "def extraer_texto(text):\n",
        "    match = re.search(r'(?<=(\\,\\s))[A-z]+(?=(\\.))', text)\n",
        "    if match:\n",
        "        return match.group(0)\n",
        "    return ''\n",
        "\n",
        "# Aplicar la funci√≥n a la columna 'texto' y crear la columna 'extracto'\n",
        "df['Title'] = df['Name'].apply(extraer_texto)\n",
        "df"
      ],
      "metadata": {
        "id": "aY5l5SPmOV9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Percentage of Missing Cabin Info per Travelling Class\n",
        "for i in [1, 2, 3]:\n",
        "  print('Pclass', i, 'No Cabin Info')\n",
        "  print('-------------------------')\n",
        "  new_df = df[df['Pclass'] == i]\n",
        "  print(len(new_df[new_df['Cabin'].isnull()])/new_df.shape[0])\n",
        "  print()"
      ],
      "metadata": {
        "id": "snEbZjICKWxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that most of the passengers of Pclass 2 and 3 had no cabin assigned. Those who did where in section D, E, F and G. First class passengers where in section A, B, C, D, E"
      ],
      "metadata": {
        "id": "MHpgLsC6M3Jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values (Percentage)\n",
        "df.isnull().sum()*100/df.shape[0]"
      ],
      "metadata": {
        "id": "hFmSFc8EW3Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, about a 20% of the data in the *Age* column is missing. Therefore, we will complete it using imputation techniques. For the 'Cabin' column, the 77% of the data is missing, thus, it will be excluded for further analysis. Rg"
      ],
      "metadata": {
        "id": "fSfZjwHIc95r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop('Cabin', axis = 1)\n",
        "df.columns"
      ],
      "metadata": {
        "id": "WE4O0lTUreKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will redefine the data types for each column according to their content."
      ],
      "metadata": {
        "id": "4oQpjjHxr9nU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correcting data types\n",
        "# Datatypes\n",
        "types = {'PassengerId':'string',\n",
        "         'Pclass':'string',\n",
        "         'Title':'string',\n",
        "         'Name':'string',\n",
        "         'Sex':'string',\n",
        "         'Ticket':'string',\n",
        "         'Embarked':'string',\n",
        "         }\n",
        "df = df.astype(types)\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "-sUK5iodlSTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical Values\n",
        "numerical = ['Age', 'SibSp', 'Parch', 'Fare']\n",
        "\n",
        "# Categorical Values\n",
        "categorical = ['Pclass', 'Sex', 'Embarked', 'Title']\n",
        "\n",
        "# ID Columns\n",
        "id = ['PassengerId', 'Name', 'Ticket']"
      ],
      "metadata": {
        "id": "QOYpF1RCXeUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Within the dataframe, we have numerical, categorical and ID variables. ID columns will not be used when implementing the model, this is because they are unique to each instance (as IDs should be), thus having no effective use to establish relations with other variables. We will only redefine *PassengerID* as the dataframe index column."
      ],
      "metadata": {
        "id": "LmMcQy4Vw2gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(id[1:], axis = 1)\n",
        "df = df.set_index(id[0])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nAAuaJOZz6Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Description of Numerical Variables**"
      ],
      "metadata": {
        "id": "tPd-6mq0nhlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Statistics***"
      ],
      "metadata": {
        "id": "JaKURqp5oKEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[numerical].describe()"
      ],
      "metadata": {
        "id": "WGuhPCuTZBDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[numerical].skew()"
      ],
      "metadata": {
        "id": "0U6xgwBWofWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All numerical values have a positive asymmetry (skewness to the right), specially the *Fare* variable."
      ],
      "metadata": {
        "id": "skUj7uH-opQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "minmax_scaling(df[numerical]).boxplot()"
      ],
      "metadata": {
        "id": "fg_fu-ZeqMqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heatmap for numerical variables\n",
        "numericalCorr = df[numerical].corr()\n",
        "heatMap(numericalCorr)"
      ],
      "metadata": {
        "id": "nq1-SQ_SrRs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### ***Age***"
      ],
      "metadata": {
        "id": "rGe69Tr6sjg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Age\"].hist()"
      ],
      "metadata": {
        "id": "khGqZ0jQskha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot per variable\n",
        "sns.catplot(pd.DataFrame(df['Age']), kind='box')"
      ],
      "metadata": {
        "id": "zf0jsIP6rJ9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Age by Sex\n",
        "sns.catplot(data=df, x=\"Sex\", y=\"Age\", kind=\"box\")"
      ],
      "metadata": {
        "id": "Hi9yU5HKsXWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Age by Pclass\n",
        "sns.catplot(data=df, x=\"Pclass\", y=\"Age\", kind=\"box\")"
      ],
      "metadata": {
        "id": "ohq_Wz0fw2pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Age by class Survived\n",
        "sns.catplot(data=df, x=\"Survived\", y=\"Age\", kind=\"box\")"
      ],
      "metadata": {
        "id": "iP-uWd_LsX32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Number of Siblings/Spouses***"
      ],
      "metadata": {
        "id": "rm1XXDHbtEWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['SibSp'].hist()"
      ],
      "metadata": {
        "id": "GfK-qqUltDjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot per variable\n",
        "sns.catplot(pd.DataFrame(df['SibSp']), kind='box')"
      ],
      "metadata": {
        "id": "NKQIel3GYhV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of SibSp by Age\n",
        "sns.catplot(data=df, x=\"SibSp\", y=\"Age\", kind=\"box\")"
      ],
      "metadata": {
        "id": "IPod80bfwCno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Number of Parents/Children***"
      ],
      "metadata": {
        "id": "-lCPKFBTtMl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Parch'].hist()"
      ],
      "metadata": {
        "id": "epXqTfIrtXqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot per variable\n",
        "sns.catplot(pd.DataFrame(df['Parch']), kind='box')"
      ],
      "metadata": {
        "id": "ABGVgcu4YlGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Parch by Age\n",
        "sns.catplot(data=df, x=\"Parch\", y=\"Age\", kind=\"box\")"
      ],
      "metadata": {
        "id": "ClZkNjFXRtxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Fare***"
      ],
      "metadata": {
        "id": "toEBD7cRtYQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Fare'].hist()"
      ],
      "metadata": {
        "id": "aGrVFgTAts69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot per variable\n",
        "sns.catplot(pd.DataFrame(df['Fare']), kind='box')"
      ],
      "metadata": {
        "id": "3Hg47NRFY00T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Outliers for Numerical Variables**"
      ],
      "metadata": {
        "id": "XSCMX3Vg9_kd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **IQR Method**"
      ],
      "metadata": {
        "id": "f73Y6CEh3QY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_outliers(variable):\n",
        "    Q1 = np.percentile(variable, 25)\n",
        "    Q3 = np.percentile(variable, 75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = variable[(variable < lower_bound) | (variable > upper_bound)]\n",
        "    return outliers\n",
        "\n",
        "fare_outliers = find_outliers(df['Fare'])\n",
        "age_outliers = find_outliers(df['Age'])\n",
        "sibsp_outliers = find_outliers(df['SibSp'])\n",
        "parch_outliers = find_outliers(df['Parch'])"
      ],
      "metadata": {
        "id": "cTo5zA_53NJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fare outliers: \\n\", fare_outliers)"
      ],
      "metadata": {
        "id": "IJpx328a513f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Age outliers: \\n\", age_outliers) # None outliers shown with the IQR Method"
      ],
      "metadata": {
        "id": "gIsqPJjS57PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"SibSp outliers: \\n\", sibsp_outliers)"
      ],
      "metadata": {
        "id": "XTBw1c3Z58pD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_num_outliers = pd.concat([fare_outliers, age_outliers, sibsp_outliers, parch_outliers], axis = 1)\n",
        "minmax_scaling(df_num_outliers).boxplot()"
      ],
      "metadata": {
        "id": "51bQrDigZA05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Parch outliers: \\n\", parch_outliers)"
      ],
      "metadata": {
        "id": "B_koGKbx6i1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Histograms"
      ],
      "metadata": {
        "id": "mM61v_nBkW8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Fare\n",
        "plt.subplot(221)\n",
        "plt.hist(df['Fare'], bins=20)\n",
        "plt.scatter(fare_outliers.index, fare_outliers.values, color='red', label='Outliers')\n",
        "plt.title('Fare Histogram with Outliers')\n",
        "plt.ylabel('Frequency')\n",
        "plt.ylim(0, 501)\n",
        "\n",
        "# SibSp\n",
        "plt.subplot(222)\n",
        "plt.hist(df['SibSp'], bins=20)\n",
        "plt.scatter(sibsp_outliers.index, sibsp_outliers.values, color='red', label='Outliers')\n",
        "plt.title('SibSp Histogram with Outliers')\n",
        "plt.ylabel('Frequency')\n",
        "plt.ylim(0, 9)\n",
        "\n",
        "# Parch\n",
        "plt.subplot(223)\n",
        "plt.hist(df['Parch'], bins=20)\n",
        "plt.scatter(parch_outliers.index, parch_outliers.values, color='red', label='Outliers')\n",
        "plt.title('Parch Histogram with Outliers')\n",
        "plt.ylabel('Frequency')\n",
        "plt.ylim(0, 7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qqLzs6Xykapx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scatter Plots"
      ],
      "metadata": {
        "id": "7kw2I17Wkksa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Age vs Fare\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.scatter(df['Age'], df['Fare'])\n",
        "plt.scatter(age_outliers.index, age_outliers.values, color='red', label='Age Outliers') # None outliers\n",
        "plt.scatter(fare_outliers.index, fare_outliers.values, color='green', label='Fare Outliers')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Fare')\n",
        "plt.legend()\n",
        "plt.title('Age vs Fare')\n",
        "\n",
        "# SibSp vs Parch\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.scatter(df['SibSp'], df['Parch'])\n",
        "plt.scatter(sibsp_outliers.index, sibsp_outliers.values, color='red', label='SibSp Outliers')\n",
        "plt.scatter(parch_outliers.index, parch_outliers.values, color='green', label='Parch Outliers')\n",
        "plt.xlabel('SibSp')\n",
        "plt.ylabel('Parch')\n",
        "plt.legend()\n",
        "plt.title('SibSp vs Parch')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aoBYyyt_kjgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Maximum Standar Deviations**"
      ],
      "metadata": {
        "id": "TYJu0lkjc5VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_num_outliers = df.copy()\n",
        "for column in numerical[1:]:\n",
        "  df_num_outliers = df_num_outliers[abs(stats.zscore(df_num_outliers[column])) < 3]\n",
        "df_num_outliers"
      ],
      "metadata": {
        "id": "LZYN0eV92G_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minmax_scaling(df_num_outliers[numerical]).boxplot()"
      ],
      "metadata": {
        "id": "lz_GzXHGiU7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Description of Categorical Variables**"
      ],
      "metadata": {
        "id": "gA-qKfzAnXZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mode for Categorical Values\n",
        "df[categorical].mode()"
      ],
      "metadata": {
        "id": "C6ZsDrrvns8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution\n",
        "columns = categorical\n",
        "for col in columns:\n",
        "  print('-------------------------------------------')\n",
        "  print(col)\n",
        "  print(df[col].value_counts())"
      ],
      "metadata": {
        "id": "8vHjHnqEYjNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "survived_df = df[df['Survived'] == 1]"
      ],
      "metadata": {
        "id": "SBfX4fXOVeOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pclass**"
      ],
      "metadata": {
        "id": "XKKS-ub-TLCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Pclass\n",
        "class_distribution = df['Pclass'].value_counts()\n",
        "ax = sns.barplot(x=class_distribution.index, y=class_distribution.values, order = ['1', '2', '3'])\n",
        "ax.set_ylim([0, 500])\n",
        "ax"
      ],
      "metadata": {
        "id": "rTKyc4jbVRkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Survivors per class\n",
        "class_distribution = survived_df['Pclass'].value_counts()\n",
        "ax = sns.barplot(x=class_distribution.index, y=class_distribution.values, order = ['1', '2', '3'])\n",
        "ax.set_ylim([0, 500])\n",
        "ax"
      ],
      "metadata": {
        "id": "7t6d_gytTKhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sex**"
      ],
      "metadata": {
        "id": "gTZZhAlDTPHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Sex\n",
        "class_distribution = df['Sex'].value_counts()\n",
        "ax = sns.barplot(x=class_distribution.index, y=class_distribution.values, order = ['female', 'male'])\n",
        "ax.set_ylim([0, 600])\n",
        "ax"
      ],
      "metadata": {
        "id": "q0B1ZhQ5WhqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Survivors per sex\n",
        "class_distribution = survived_df['Sex'].value_counts()\n",
        "ax = sns.barplot(x=class_distribution.index, y=class_distribution.values, order = ['female', 'male'])\n",
        "ax.set_ylim([0, 500])\n",
        "ax"
      ],
      "metadata": {
        "id": "DdTiAYwSWngO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Embarked**"
      ],
      "metadata": {
        "id": "nZ3kujtJTTYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Embarked\n",
        "class_distribution = df['Embarked'].value_counts()\n",
        "ax = sns.barplot(x=class_distribution.index, y=class_distribution.values, order = ['C', 'Q', 'S'])\n",
        "ax.set_ylim([0, 700])\n",
        "ax"
      ],
      "metadata": {
        "id": "-dns71XJTWIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Survivors per category\n",
        "class_distribution = survived_df['Embarked'].value_counts()\n",
        "ax = sns.barplot(x=class_distribution.index, y=class_distribution.values, order = ['C', 'Q', 'S'])\n",
        "ax.set_ylim([0, 700])\n",
        "ax"
      ],
      "metadata": {
        "id": "aOc93ot1X2Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Title**"
      ],
      "metadata": {
        "id": "JpLt25ZTTW66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Title\n",
        "class_distribution = df['Title'].value_counts()\n",
        "ax = sns.barplot(x=class_distribution.index, y=class_distribution.values)\n",
        "ax.set_ylim([0, 600])\n",
        "ax"
      ],
      "metadata": {
        "id": "kEvE1fNAYAJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Survivors per Title\n",
        "class_distribution = survived_df['Title'].value_counts()\n",
        "ax = sns.barplot(x=class_distribution.index, y=class_distribution.values)\n",
        "ax.set_ylim([0, 600])\n",
        "ax"
      ],
      "metadata": {
        "id": "i8MnbM_VYHRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Survived**"
      ],
      "metadata": {
        "id": "uFEb4BaFayER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of Survived\n",
        "class_distribution = df['Survived'].value_counts()\n",
        "ax = sns.barplot(x=class_distribution.index, y=class_distribution.values)\n",
        "ax.set_ylim([0, 600])\n",
        "ax"
      ],
      "metadata": {
        "id": "B-WaKas6axex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Outliers for Categorical Variables**"
      ],
      "metadata": {
        "id": "2H19Qr0Ps31W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deleting the values that have a frequency less than 10 in \"Title\"\n",
        "categorical_df = df_num_outliers.copy()\n",
        "print(\"Values after outliers cleaning\")\n",
        "print(\"-------------------------------------\")\n",
        "categories_freq = categorical_df['Title'].value_counts()\n",
        "frecuencia_df = pd.DataFrame({'Categoria': categories_freq.index, 'Frecuencia': categories_freq.values})\n",
        "\n",
        "# Creating list of categories with less than 10 instances\n",
        "delete_categories = frecuencia_df[frecuencia_df['Frecuencia'] <= 10]['Categoria']\n",
        "\n",
        "# Deleting instances with irrelevant categories\n",
        "df_cat_outliers = categorical_df[~categorical_df['Title'].isin(delete_categories)]\n",
        "df_cat_outliers['Title'].value_counts()\n"
      ],
      "metadata": {
        "id": "ZmDHEVGls93t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As it can be noticed, there are some categorical values that only have one person or it's used only once or seven times. For example, in \"Title\" there is only one \"Sir\", one \"Lady\", etc. Those values won't be useful for the analysis and that's why these rows were deleted. In the other categorical variables such as \"Embarked\" and \"Sex\" the values frequency were all greater than 10, so we didn't delete any row in this section."
      ],
      "metadata": {
        "id": "rShNeGlcAkzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Encoding of Categorical Values**"
      ],
      "metadata": {
        "id": "SF_0cWgdrfo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cat_outliers['Title'].value_counts()"
      ],
      "metadata": {
        "id": "o3MtPXxbC1rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoding(dataframe):\n",
        "  # Get Dummies\n",
        "  result = pd.DataFrame()\n",
        "  for column in categorical:\n",
        "    encoded = pd.get_dummies(dataframe[column])\n",
        "\n",
        "    # Renaming variables to know their origin column\n",
        "    for name in encoded.columns:\n",
        "      encoded = encoded.rename(columns = {name: f\"{column}_{name}\"})\n",
        "\n",
        "    result = pd.concat([result, encoded], axis=1)\n",
        "\n",
        "  df_encoded = pd.concat([dataframe, result], axis = 1)\n",
        "  df_encoded = df_encoded.drop(categorical, axis = 1)\n",
        "\n",
        "  return df_encoded\n",
        "\n",
        "df_encoded = one_hot_encoding(df_cat_outliers)\n",
        "df_encoded\n"
      ],
      "metadata": {
        "id": "lq1kFzZLrfUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Missing Values (Imputation Techniques)**"
      ],
      "metadata": {
        "id": "2Q6d9hrk-C3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When trying to train a model and make predictions based on the training set it is of vital importance to have good quality data as well as a good amount of it. This is why we are in dire need to impute the missing data on the ages, a feature that we consider to be determinant. For this process we will try 3 methods to impute it and then try to make predictions on the training set in order to evaluate which imputing method was the most effective."
      ],
      "metadata": {
        "id": "qlTKE9-0Rav1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **KNN Imputation**"
      ],
      "metadata": {
        "id": "KZsjIQenXkMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "def knn_imputation(df):\n",
        "    features = df[['Age', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'SibSp', 'Fare']]\n",
        "    imputer = KNNImputer(n_neighbors=5)\n",
        "    imputed_values = imputer.fit_transform(features)\n",
        "    df['Age'] = imputed_values[:, 0]\n",
        "    return df\n",
        "\n",
        "df_encoded_knn = knn_imputation(df_encoded.copy())\n"
      ],
      "metadata": {
        "id": "oH3LfH8xPlQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Random Forest Imputation**"
      ],
      "metadata": {
        "id": "HuwKR6IpcntY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "def model_based_imputation(df):\n",
        "    train_data = df.dropna(subset=['Age'])\n",
        "    test_data = df[df['Age'].isnull()]\n",
        "\n",
        "    X_train = train_data[['Pclass_1', 'Pclass_2', 'Pclass_3', 'SibSp', 'Fare']]\n",
        "    y_train = train_data['Age']\n",
        "    X_test = test_data[['Pclass_1', 'Pclass_2', 'Pclass_3', 'SibSp', 'Fare']]\n",
        "\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    predicted_ages = model.predict(X_test)\n",
        "\n",
        "    df.loc[df['Age'].isnull(), 'Age'] = predicted_ages\n",
        "    return df\n",
        "\n",
        "df_encoded_rf = model_based_imputation(df_encoded.copy())\n"
      ],
      "metadata": {
        "id": "tq4u9PKncyJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mean Imputation**"
      ],
      "metadata": {
        "id": "o8dj5eYBc_8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_imputation(df):\n",
        "    df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
        "    return df\n",
        "\n",
        "df_encoded_mean = mean_imputation(df_encoded.copy())\n"
      ],
      "metadata": {
        "id": "nwJ0ltb7dLT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Median Imputation**"
      ],
      "metadata": {
        "id": "lLagTiGpq-b1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_age = df_cat_outliers.copy()\n",
        "df_age = df_age[df_age['Age'].notna()]\n",
        "\n",
        "# Create a figure with a 2x2 grid of subplots\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 8))\n",
        "\n",
        "# Flatten the 2x2 array of axes for easier indexing\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Iterate through titles and create histograms in separate subplots\n",
        "for idx, title in enumerate(df_age['Title'].unique()):\n",
        "    title_data = df_age[df_age['Title'] == title]\n",
        "    ax = axes[idx]\n",
        "    ax.hist(title_data['Age'], bins=20, alpha=0.5)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Age')\n",
        "    ax.set_ylabel('Frequency')\n",
        "\n",
        "# Adjust layout to prevent overlapping titles and labels\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MKsVKtyJrBDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for title in df_age['Title'].unique():\n",
        "  title_df = df_age [df_age['Title'] == title]\n",
        "  print(title, ' ------------------------------')\n",
        "  print('Skewness: ', title_df['Age'].skew())\n",
        "  ad_test = 0\n",
        "  print('Anderson-Darling: ', stats.anderson(title_df['Age']))"
      ],
      "metadata": {
        "id": "dU-L0E28rL_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the Anderson-Darling, we can discard the null hypothesis (normality of the data) for every *Title* value, except for *Mrs*. Due to this, and the skewness in every category, it is better to use the median for imputation of values instead of the mean.\n",
        "\n",
        "We will then compute the median per category and fill the missing values according to that:"
      ],
      "metadata": {
        "id": "pCBRlgtArOdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill NaN values in 'Age' according to 'Title\n",
        "df_median = df_cat_outliers.copy()\n",
        "for title in df_median['Title'].unique():\n",
        "  median = df_median[df_median['Title'] == title]['Age'].median()\n",
        "  df_median.loc[(df_median['Title'] == title) & (df_median['Age'].isna()), 'Age'] = median\n",
        "\n",
        "df_encoded_median = one_hot_encoding(df_median)\n",
        "df_encoded_median"
      ],
      "metadata": {
        "id": "UBSxPA_rrQy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded_median.columns"
      ],
      "metadata": {
        "id": "oRttIKMHBXH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation of Methods**\n",
        "\n",
        "For the evaluation of these methods, a logistic regression was applied in order to see the performance of each method."
      ],
      "metadata": {
        "id": "lOophX3LdZhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def evaluate_model(X, y):\n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    return cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
        "\n",
        "y = df_encoded['Survived']\n",
        "\n",
        "X_encoded = df_encoded.drop('Survived', axis=1)\n",
        "\n",
        "X_knn = knn_imputation(X_encoded.copy())\n",
        "knn_accuracy = evaluate_model(X_knn, y)\n",
        "\n",
        "X_rf = model_based_imputation(X_encoded.copy())\n",
        "rf_accuracy = evaluate_model(X_rf, y)\n",
        "\n",
        "X_mean = mean_imputation(X_encoded.copy())\n",
        "mean_accuracy = evaluate_model(X_mean, y)\n",
        "\n",
        "X_median = df_encoded_median[df_encoded_median.columns[1:]]\n",
        "median_accuracy = evaluate_model(X_median, y)\n",
        "\n",
        "results = {\n",
        "    'KNN Imputation': knn_accuracy,\n",
        "    'Random Forest Imputation': rf_accuracy,\n",
        "    'Mean Imputation': mean_accuracy,\n",
        "    'Median Imputation': median_accuracy\n",
        "}\n",
        "\n",
        "\n",
        "print(f\"{'Imputation Method':<25} | {'Accuracy':<10}\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "for method, accuracy in results.items():\n",
        "    print(f\"{method:<25} | {accuracy:<10.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "K2IOum6jdgb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see above, the results are very close to each other and if ran several times the experiment the deviation between the results given by the random forest are the beter than KNN Imputation and Mean Imputation but not by much. Median imputation is the method with best results, however, in order to enhance the precission of our predictions at the maximum, the Random Forest method has been chosen for the data imputation for the ages."
      ],
      "metadata": {
        "id": "M7bEeKASeH_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Duplicates**"
      ],
      "metadata": {
        "id": "4DWrNs_6nAFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = df_encoded_rf.drop_duplicates(keep = 'first')\n",
        "new_df"
      ],
      "metadata": {
        "id": "6-SeCiiim_I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Correlation Among All Variables**"
      ],
      "metadata": {
        "id": "3-6OtKgC925Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spearman coeficient for relations between categorical and numerical features\n",
        "corr = new_df.corr(method = 'spearman')\n",
        "sns.heatmap(corr)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ewPVXkBPZ2_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using only the absolute values of the heat map, we can more easily visualize the strength of the relation. To know whether it is negative or positive, we can check the correlation matrix, or the heat map above."
      ],
      "metadata": {
        "id": "CcLOT007_sf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(abs(corr), cmap=\"gray_r\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h2shUXmm-pTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, some of the pairs of variables that have a strong correlation with the variable *Survived* are *Title_Mr* (negative), *Sex_female* (positive) and *Sex_male* (negative). A less strong correlation, but still existant, can be observed between *Survival* and *Fare*, *Pclass_3*, *Title_Ms* and *Title_Mrs*.\n",
        "\n",
        "Due to the strong correlation between some of those features (such as *Title_Mr* and *Sex_male*) we can opt to eliminate one of them in the Feature Selection process.\n"
      ],
      "metadata": {
        "id": "oWAakS69A-jJ"
      }
    }
  ]
}